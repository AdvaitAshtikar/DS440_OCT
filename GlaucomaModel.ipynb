{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: torch in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (75.5.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: kagglehub in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (10.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (8.29.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kagglehub) (4.67.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.45.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: rich in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\tonyz\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\tonyz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets torch torchvision setuptools tensorflow kagglehub opencv-python numpy pandas scipy scikit-learn pillow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonyz\\AppData\\Local\\Temp\\ipykernel_31924\\165285209.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('unet_model.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.216:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from GlaucomaModel import UNet, vertical_cup_to_disc_ratio, refine_seg\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
    "\n",
    "# Load the pre-trained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(n_channels=3, n_classes=2).to(device)\n",
    "checkpoint = torch.load('unet_model.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Threshold for vCDR classification\n",
    "vCDR_threshold = 0.6\n",
    "\n",
    "# Preprocessing function for input images\n",
    "def preprocess(image):\n",
    "    # Ensure the image is in RGB format, regardless of original format\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "    # Resize image to a fixed size while preserving aspect ratio and padding\n",
    "    size = (256, 256)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),  # Resize the shorter side to 256\n",
    "        transforms.CenterCrop(size),  # Crop the center to get a 256x256 image\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalization for pre-trained models\n",
    "    ])\n",
    "\n",
    "    return transform(image)\n",
    "\n",
    "@app.route('/api/calculate_vcdr', methods=['POST'])\n",
    "def calculate_vcdr():\n",
    "    if 'images' not in request.files:\n",
    "        return jsonify({\"error\": \"No files uploaded\"}), 400\n",
    "\n",
    "    files = request.files.getlist('images')\n",
    "    if not files:\n",
    "        return jsonify({\"error\": \"No files received\"}), 400\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = Image.open(file).convert('RGB')\n",
    "            input_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "            # Forward pass through the model\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_tensor)\n",
    "\n",
    "            # Get segmentation predictions for OD and OC\n",
    "            pred_od = refine_seg((logits[:, 0, :, :] >= 0.5).type(torch.int8).cpu()).to(device)\n",
    "            pred_oc = refine_seg((logits[:, 1, :, :] >= 0.5).type(torch.int8).cpu()).to(device)\n",
    "\n",
    "            # Compute vCDR\n",
    "            pred_vCDR = vertical_cup_to_disc_ratio(pred_od.cpu().numpy(), pred_oc.cpu().numpy())[0]\n",
    "\n",
    "            # Classify based on vCDR threshold\n",
    "            predicted_label = \"Glaucoma\" if pred_vCDR > vCDR_threshold else \"No Glaucoma\"\n",
    "\n",
    "            # Append the result\n",
    "            results.append({\n",
    "                \"file\": file.filename,\n",
    "                \"vCDR\": f\"{pred_vCDR:.2f}\",\n",
    "                \"prediction\": predicted_label\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file.filename}: {str(e)}\")\n",
    "            results.append({\"file\": file.filename, \"error\": str(e)})\n",
    "\n",
    "    return jsonify({\"results\": results}), 200\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FwlA9iGSMBXq",
    "outputId": "cd8481cc-69a5-4800-9638-c5743c71386d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import kagglehub\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.ndimage import label\n",
    "from tensorflow import keras\n",
    "from keras._tf_keras.keras.preprocessing.image import array_to_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbDbgucCMHSj"
   },
   "outputs": [],
   "source": [
    "class GlaucomaDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', output_size=(256, 256)):\n",
    "        self.output_size = output_size\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.images = []\n",
    "        self.segs = []\n",
    "\n",
    "        for direct in self.root_dir:\n",
    "            self.image_filenames = []\n",
    "            for path in os.listdir(os.path.join(direct, \"Images_Square\")):\n",
    "                if not path.startswith('.'):\n",
    "                    self.image_filenames.append(path)\n",
    "\n",
    "            for k in range(len(self.image_filenames)):\n",
    "                try:\n",
    "                    print(f'Loading {split} image {k}/{len(self.image_filenames)}...', end='\\r')\n",
    "                    img_name = os.path.join(direct, \"Images_Square\", self.image_filenames[k])\n",
    "                    img = np.array(Image.open(img_name).convert('RGB'))\n",
    "                    img = transforms.functional.to_tensor(img)\n",
    "                    img = transforms.functional.resize(img, output_size, interpolation=Image.BILINEAR)\n",
    "                    self.images.append(img)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError loading image {self.image_filenames[k]}: {e}\")\n",
    "                    continue \n",
    "\n",
    "            if split != 'test':\n",
    "                for k in range(len(self.image_filenames)):\n",
    "                    try:\n",
    "                        print(f'Loading {split} segmentation {k}/{len(self.image_filenames)}...', end='\\r')\n",
    "                        seg_name = os.path.join(direct, \"Masks_Square\", self.image_filenames[k][:-3] + \"png\")\n",
    "                        mask = np.array(Image.open(seg_name, mode='r'))\n",
    "                        od = (mask == 1.).astype(np.float32)\n",
    "                        oc = (mask == 2.).astype(np.float32)\n",
    "                        od = torch.from_numpy(od[None, :, :])\n",
    "                        oc = torch.from_numpy(oc[None, :, :])\n",
    "                        od = transforms.functional.resize(od, output_size, interpolation=Image.NEAREST)\n",
    "                        oc = transforms.functional.resize(oc, output_size, interpolation=Image.NEAREST)\n",
    "                        self.segs.append(torch.cat([od, oc], dim=0))\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\nError loading segmentation for {self.image_filenames[k]}: {e}\")\n",
    "                        continue \n",
    "\n",
    "            print(f'Successfully loaded {split} dataset.', ' ' * 50)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        if self.split == 'test':\n",
    "            return img\n",
    "        else:\n",
    "            seg = self.segs[idx]\n",
    "            return img, seg\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_dir, json_path, output_size=(256, 256)):\n",
    "        self.output_size = output_size\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "        # Load images\n",
    "        self.image_filenames = []\n",
    "        for path in os.listdir(image_dir):\n",
    "            if not path.startswith('.'):\n",
    "                self.image_filenames.append(path)\n",
    "\n",
    "        # Load ground truth glaucoma labels from JSON file\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.ground_truth = json.load(f)\n",
    "\n",
    "        # Create a dictionary mapping from image filename to label\n",
    "        self.filename_to_label = {}\n",
    "        for item in self.ground_truth.values():\n",
    "            filename = item[\"ImgName\"]\n",
    "            label = item[\"Label\"]\n",
    "            self.filename_to_label[filename] = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_name = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "            img = np.array(Image.open(img_name).convert('RGB'))\n",
    "            img = transforms.functional.to_tensor(img)\n",
    "            img = transforms.functional.resize(img, self.output_size, interpolation=Image.BILINEAR)\n",
    "            file_id = self.image_filenames[idx]\n",
    "            has_glaucoma = self.filename_to_label.get(file_id, None)\n",
    "\n",
    "            if has_glaucoma is None:\n",
    "                raise KeyError(f\"Glaucoma label for '{file_id}' not found in ground_truth.\")\n",
    "\n",
    "            return img, int(has_glaucoma)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError loading file {self.image_filenames[idx]}: {e}\")\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORs9x_eQMP6M"
   },
   "outputs": [],
   "source": [
    "EPS = 1e-7\n",
    "\n",
    "def compute_dice_coef(input, target):\n",
    "    batch_size = input.shape[0]\n",
    "    return sum([dice_coef_sample(input[k, :, :], target[k, :, :]) for k in range(batch_size)]) / batch_size\n",
    "\n",
    "def dice_coef_sample(input, target):\n",
    "    iflat = input.contiguous().view(-1)\n",
    "    tflat = target.contiguous().view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return (2. * intersection) / (iflat.sum() + tflat.sum())\n",
    "\n",
    "def vertical_diameter(binary_segmentation):\n",
    "    vertical_axis_diameter = np.sum(binary_segmentation, axis=1)\n",
    "    diameter = np.max(vertical_axis_diameter, axis=1)\n",
    "    return diameter\n",
    "\n",
    "def vertical_cup_to_disc_ratio(od, oc):\n",
    "    cup_diameter = vertical_diameter(oc)\n",
    "    disc_diameter = vertical_diameter(od)\n",
    "    return cup_diameter / (disc_diameter + EPS)\n",
    "\n",
    "def compute_vCDR_error(pred_od, pred_oc, gt_od, gt_oc):\n",
    "    pred_vCDR = vertical_cup_to_disc_ratio(pred_od, pred_oc)\n",
    "    gt_vCDR = vertical_cup_to_disc_ratio(gt_od, gt_oc)\n",
    "    vCDR_err = np.mean(np.abs(gt_vCDR - pred_vCDR))\n",
    "    return vCDR_err, pred_vCDR, gt_vCDR\n",
    "\n",
    "def refine_seg(pred):\n",
    "    np_pred = pred.numpy()\n",
    "    largest_ccs = []\n",
    "    for i in range(np_pred.shape[0]):\n",
    "        labeled, ncomponents = label(np_pred[i, :, :])\n",
    "        bincounts = np.bincount(labeled.flat)[1:]\n",
    "        largest_cc = labeled == np.argmax(bincounts) + 1 if len(bincounts) != 0 else labeled == 0\n",
    "        largest_ccs.append(torch.tensor(largest_cc, dtype=torch.float32))\n",
    "    return torch.stack(largest_ccs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JPKh7BDMR7Y"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=2):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.epoch = 0\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        self.down5 = Down(1024, 2048)\n",
    "        factor = 2\n",
    "        self.down6 = Down(2048, 4096 // factor)\n",
    "        self.up1 = Up(4096, 2048 // factor)\n",
    "        self.up2 = Up(2048, 1024 // factor)\n",
    "        self.up3 = Up(1024, 512 // factor)\n",
    "        self.up4 = Up(512, 256 // factor)\n",
    "        self.up5 = Up(256, 128 // factor)\n",
    "        self.up6 = Up(128, 64)\n",
    "        self.output_layer = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x6 = self.down5(x5)\n",
    "        x7 = self.down6(x6)\n",
    "        out = self.up1(x7, x6)\n",
    "        out = self.up2(out, x5)\n",
    "        out = self.up3(out, x4)\n",
    "        out = self.up4(out, x3)\n",
    "        out = self.up5(out, x2)\n",
    "        out = self.up6(out, x1)\n",
    "        return torch.sigmoid(self.output_layer(out))\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(nn.MaxPool2d(2), DoubleConv(in_channels, out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY, diffX = x2.size()[2] - x1.size()[2], x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        return self.conv(torch.cat([x2, x1], dim=1))\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNdp0FYGRz_I",
    "outputId": "670815f8-3f78-4778-9bbe-141e4d1df501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded train dataset.                                                   \n",
      "Successfully loaded train dataset.                                                   \n",
      "Successfully loaded val dataset.                                                   \n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"arnavjain1/glaucoma-datasets\")\n",
    "train_dir = os.path.join(path, 'G1020')\n",
    "train_dir2 = os.path.join(path, 'ORIGA')\n",
    "train_dirs = [train_dir, train_dir2]\n",
    "val_dir = [os.path.join(path, 'REFUGE')]\n",
    "train_set = GlaucomaDataset(train_dirs, split='train')\n",
    "val_set = GlaucomaDataset(val_dir, split='val')\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonyz\\AppData\\Local\\Temp\\ipykernel_19992\\2048598210.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('unet_model.pth', map_location=device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Model, Loss, Optimizer\n",
    "model = UNet(n_channels=3, n_classes=2).to(device)\n",
    "seg_loss = torch.nn.BCELoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "checkpoint = torch.load('unet_model.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# Training and Validation Loop\n",
    "nb_train_batches = len(train_loader)\n",
    "nb_val_batches = len(val_loader)\n",
    "nb_iter = 0\n",
    "best_val_auc = 0.\n",
    "iters = list(range(1, 10))\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDQ3bI40MTwc",
    "outputId": "1fd86b40-387e-41bd-a675-647157c17b3e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-666b9d55cf26>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('unet_model.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1                                                  \n",
      "Loss: 0.0050 (train), 0.0092 (val)\n",
      "Dice Score - OD segmentation: 0.8919 (train), 0.7881 (val)\n",
      "Dice Score - OC segmentation: 0.7498 (train), 0.8014 (val)\n",
      "vCDR error: 1.0212 (train), 0.1764 (val)\n",
      "Epoch 2                                                  \n",
      "Loss: 0.0043 (train), 0.0080 (val)\n",
      "Dice Score - OD segmentation: 0.8987 (train), 0.7908 (val)\n",
      "Dice Score - OC segmentation: 0.7553 (train), 0.8407 (val)\n",
      "vCDR error: 1.0104 (train), 0.1537 (val)\n",
      "Epoch 3                                                  \n",
      "Loss: 0.0037 (train), 0.0077 (val)\n",
      "Dice Score - OD segmentation: 0.9069 (train), 0.8185 (val)\n",
      "Dice Score - OC segmentation: 0.7617 (train), 0.8307 (val)\n",
      "vCDR error: 1.0112 (train), 0.1717 (val)\n",
      "Epoch 4                                                  \n",
      "Loss: 0.0034 (train), 0.0086 (val)\n",
      "Dice Score - OD segmentation: 0.9112 (train), 0.7827 (val)\n",
      "Dice Score - OC segmentation: 0.7712 (train), 0.8339 (val)\n",
      "vCDR error: 0.9739 (train), 0.1421 (val)\n",
      "Epoch 5                                                  \n",
      "Loss: 0.0032 (train), 0.0071 (val)\n",
      "Dice Score - OD segmentation: 0.9153 (train), 0.8379 (val)\n",
      "Dice Score - OC segmentation: 0.7752 (train), 0.8342 (val)\n",
      "vCDR error: 1.0253 (train), 0.1049 (val)\n",
      "Epoch 6                                                  \n",
      "Loss: 0.0030 (train), 0.0075 (val)\n",
      "Dice Score - OD segmentation: 0.9177 (train), 0.8165 (val)\n",
      "Dice Score - OC segmentation: 0.7792 (train), 0.8410 (val)\n",
      "vCDR error: 1.0159 (train), 0.1212 (val)\n",
      "Epoch 7                                                  \n",
      "Loss: 0.0029 (train), 0.0072 (val)\n",
      "Dice Score - OD segmentation: 0.9193 (train), 0.8318 (val)\n",
      "Dice Score - OC segmentation: 0.7841 (train), 0.8373 (val)\n",
      "vCDR error: 1.0040 (train), 0.2106 (val)\n",
      "Epoch 8                                                  \n",
      "Loss: 0.0028 (train), 0.0090 (val)\n",
      "Dice Score - OD segmentation: 0.9225 (train), 0.8023 (val)\n",
      "Dice Score - OC segmentation: 0.7835 (train), 0.8359 (val)\n",
      "vCDR error: 0.9961 (train), 0.1054 (val)\n",
      "Epoch 9                                                  \n",
      "Loss: 0.0028 (train), 0.0082 (val)\n",
      "Dice Score - OD segmentation: 0.9222 (train), 0.8141 (val)\n",
      "Dice Score - OC segmentation: 0.7873 (train), 0.8436 (val)\n",
      "vCDR error: 1.0792 (train), 0.1183 (val)\n",
      "Epoch 10                                                  \n",
      "Loss: 0.0031 (train), 0.0075 (val)\n",
      "Dice Score - OD segmentation: 0.9146 (train), 0.8299 (val)\n",
      "Dice Score - OC segmentation: 0.7793 (train), 0.8230 (val)\n",
      "vCDR error: 1.0039 (train), 0.1154 (val)\n",
      "Epoch 11                                                  \n",
      "Loss: 0.0027 (train), 0.0081 (val)\n",
      "Dice Score - OD segmentation: 0.9236 (train), 0.8259 (val)\n",
      "Dice Score - OC segmentation: 0.7885 (train), 0.8142 (val)\n",
      "vCDR error: 1.0122 (train), 0.2381 (val)\n",
      "Epoch 12                                                  \n",
      "Loss: 0.0028 (train), 0.0087 (val)\n",
      "Dice Score - OD segmentation: 0.9219 (train), 0.8176 (val)\n",
      "Dice Score - OC segmentation: 0.7864 (train), 0.8319 (val)\n",
      "vCDR error: 1.0029 (train), 0.1263 (val)\n",
      "Epoch 13                                                  \n",
      "Loss: 0.0023 (train), 0.0098 (val)\n",
      "Dice Score - OD segmentation: 0.9334 (train), 0.7965 (val)\n",
      "Dice Score - OC segmentation: 0.7992 (train), 0.8348 (val)\n",
      "vCDR error: 1.0335 (train), 0.1639 (val)\n",
      "Epoch 14                                                  \n",
      "Loss: 0.0023 (train), 0.0085 (val)\n",
      "Dice Score - OD segmentation: 0.9333 (train), 0.8324 (val)\n",
      "Dice Score - OC segmentation: 0.7976 (train), 0.8343 (val)\n",
      "vCDR error: 1.0265 (train), 0.0972 (val)\n",
      "Epoch 15                                                  \n",
      "Loss: 0.0021 (train), 0.0086 (val)\n",
      "Dice Score - OD segmentation: 0.9385 (train), 0.8301 (val)\n",
      "Dice Score - OC segmentation: 0.8055 (train), 0.8435 (val)\n",
      "vCDR error: 1.0157 (train), 0.1116 (val)\n",
      "Epoch 16                                                  \n",
      "Loss: 0.0020 (train), 0.0095 (val)\n",
      "Dice Score - OD segmentation: 0.9412 (train), 0.8167 (val)\n",
      "Dice Score - OC segmentation: 0.8062 (train), 0.8354 (val)\n",
      "vCDR error: 1.0236 (train), 0.1302 (val)\n",
      "Epoch 17                                                  \n",
      "Loss: 0.0019 (train), 0.0101 (val)\n",
      "Dice Score - OD segmentation: 0.9445 (train), 0.8167 (val)\n",
      "Dice Score - OC segmentation: 0.8097 (train), 0.8418 (val)\n",
      "vCDR error: 1.0258 (train), 0.1298 (val)\n",
      "Epoch 18                                                  \n",
      "Loss: 0.0018 (train), 0.0097 (val)\n",
      "Dice Score - OD segmentation: 0.9461 (train), 0.8179 (val)\n",
      "Dice Score - OC segmentation: 0.8131 (train), 0.8314 (val)\n",
      "vCDR error: 1.0285 (train), 0.1133 (val)\n",
      "Epoch 19                                                  \n",
      "Loss: 0.0018 (train), 0.0100 (val)\n",
      "Dice Score - OD segmentation: 0.9484 (train), 0.8234 (val)\n",
      "Dice Score - OC segmentation: 0.8128 (train), 0.8366 (val)\n",
      "vCDR error: 1.0229 (train), 0.1062 (val)\n",
      "Epoch 20                                                  \n",
      "Loss: 0.0017 (train), 0.0101 (val)\n",
      "Dice Score - OD segmentation: 0.9498 (train), 0.8259 (val)\n",
      "Dice Score - OC segmentation: 0.8150 (train), 0.8366 (val)\n",
      "vCDR error: 1.0218 (train), 0.1110 (val)\n",
      "Epoch 21                                                  \n",
      "Loss: 0.0017 (train), 0.0108 (val)\n",
      "Dice Score - OD segmentation: 0.9516 (train), 0.8230 (val)\n",
      "Dice Score - OC segmentation: 0.8164 (train), 0.8340 (val)\n",
      "vCDR error: 1.0079 (train), 0.1123 (val)\n",
      "Epoch 22                                                  \n",
      "Loss: 0.0016 (train), 0.0122 (val)\n",
      "Dice Score - OD segmentation: 0.9529 (train), 0.7952 (val)\n",
      "Dice Score - OC segmentation: 0.8176 (train), 0.8392 (val)\n",
      "vCDR error: 1.0343 (train), 0.1459 (val)\n",
      "Epoch 23                                                  \n",
      "Loss: 0.0016 (train), 0.0103 (val)\n",
      "Dice Score - OD segmentation: 0.9541 (train), 0.8254 (val)\n",
      "Dice Score - OC segmentation: 0.8189 (train), 0.8410 (val)\n",
      "vCDR error: 1.0055 (train), 0.1194 (val)\n",
      "Epoch 24                                                  \n",
      "Loss: 0.0015 (train), 0.0117 (val)\n",
      "Dice Score - OD segmentation: 0.9563 (train), 0.8192 (val)\n",
      "Dice Score - OC segmentation: 0.8207 (train), 0.8450 (val)\n",
      "vCDR error: 1.0272 (train), 0.1113 (val)\n",
      "Epoch 25                                                  \n",
      "Loss: 0.0015 (train), 0.0108 (val)\n",
      "Dice Score - OD segmentation: 0.9571 (train), 0.8304 (val)\n",
      "Dice Score - OC segmentation: 0.8213 (train), 0.8491 (val)\n",
      "vCDR error: 1.0008 (train), 0.1058 (val)\n"
     ]
    }
   ],
   "source": [
    "while model.epoch < num_epochs:\n",
    "    # Accumulators\n",
    "    train_vCDRs, val_vCDRs = [], []\n",
    "    train_loss, val_loss = 0., 0.\n",
    "    train_dsc_od, val_dsc_od = 0., 0.\n",
    "    train_dsc_oc, val_dsc_oc = 0., 0.\n",
    "    train_vCDR_error, val_vCDR_error = 0., 0.\n",
    "\n",
    "    model.train()\n",
    "    train_data = iter(train_loader)\n",
    "    for k in range(nb_train_batches):\n",
    "        imgs, seg_gts = next(train_data)\n",
    "        imgs, seg_gts = imgs.to(device), seg_gts.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(imgs)\n",
    "        loss = seg_loss(logits, seg_gts)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() / nb_train_batches\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Compute segmentation metric\n",
    "            pred_od = refine_seg((logits[:, 0, :, :] >= 0.5).type(torch.int8).cpu()).to(device)\n",
    "            pred_oc = refine_seg((logits[:, 1, :, :] >= 0.5).type(torch.int8).cpu()).to(device)\n",
    "            gt_od = seg_gts[:, 0, :, :].type(torch.int8)\n",
    "            gt_oc = seg_gts[:, 1, :, :].type(torch.int8)\n",
    "            dsc_od = compute_dice_coef(pred_od, gt_od)\n",
    "            dsc_oc = compute_dice_coef(pred_oc, gt_oc)\n",
    "            train_dsc_od += dsc_od.item() / nb_train_batches\n",
    "            train_dsc_oc += dsc_oc.item() / nb_train_batches\n",
    "            vCDR_error, pred_vCDR, gt_vCDR = compute_vCDR_error(pred_od.cpu().numpy(), pred_oc.cpu().numpy(), gt_od.cpu().numpy(), gt_oc.cpu().numpy())\n",
    "            train_vCDRs += pred_vCDR.tolist()\n",
    "            train_vCDR_error += vCDR_error / nb_train_batches\n",
    "        nb_iter += 1\n",
    "        print('Epoch {}, iter {}/{}, loss {:.6f}'.format(model.epoch + 1, k + 1, nb_train_batches, loss.item()) + ' ' * 20,\n",
    "              end='\\r')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k, (imgs, seg_gts) in enumerate(val_loader):\n",
    "            imgs, seg_gts = imgs.to(device), seg_gts.to(device)\n",
    "            logits = model(imgs)\n",
    "            val_loss += seg_loss(logits, seg_gts).item() / nb_val_batches\n",
    "            val_losses.append(val_loss)\n",
    "            print('Validation iter {}/{}'.format(k + 1, nb_val_batches) + ' ' * 50,\n",
    "                  end='\\r')\n",
    "            pred_od = refine_seg((logits[:, 0, :, :] >= 0.5).type(torch.int8).cpu()).to(device)\n",
    "            pred_oc = refine_seg((logits[:, 1, :, :] >= 0.5).type(torch.int8).cpu()).to(device)\n",
    "            gt_od = seg_gts[:, 0, :, :].type(torch.int8)\n",
    "            gt_oc = seg_gts[:, 1, :, :].type(torch.int8)\n",
    "            dsc_od = compute_dice_coef(pred_od, gt_od)\n",
    "            dsc_oc = compute_dice_coef(pred_oc, gt_oc)\n",
    "            val_dsc_od += dsc_od.item() / nb_val_batches\n",
    "            val_dsc_oc += dsc_oc.item() / nb_val_batches\n",
    "\n",
    "            vCDR_error, pred_vCDR, gt_vCDR = compute_vCDR_error(pred_od.cpu().numpy(), pred_oc.cpu().numpy(), gt_od.cpu().numpy(), gt_oc.cpu().numpy())\n",
    "            val_vCDRs += pred_vCDR.tolist()\n",
    "            val_vCDR_error += vCDR_error / nb_val_batches\n",
    "\n",
    "    print('Epoch {}'.format(model.epoch + 1) + ' ' * 50)\n",
    "    print('Loss: {:.4f} (train), {:.4f} (val)'.format(train_loss, val_loss))\n",
    "    print('Dice Score - OD segmentation: {:.4f} (train), {:.4f} (val)'.format(train_dsc_od, val_dsc_od))\n",
    "    print('Dice Score - OC segmentation: {:.4f} (train), {:.4f} (val)'.format(train_dsc_oc, val_dsc_oc))\n",
    "    print('vCDR error: {:.4f} (train), {:.4f} (val)'.format(train_vCDR_error, val_vCDR_error))\n",
    "    # End of epoch\n",
    "    model.epoch += 1\n",
    "\n",
    "save_path = '/content/unet_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "wXFXULDMMeNW",
    "outputId": "940cc135-fe72-47f4-9ca5-4f92bdcd417d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonyz\\AppData\\Local\\Temp\\ipykernel_19992\\2306310973.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('unet_model.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for Glaucoma Classification: 89.50%\n",
      "True Positives (TP): 33\n",
      "True Negatives (TN): 325\n",
      "False Positives (FP): 35\n",
      "False Negatives (FN): 7\n",
      "Predictions have been saved to glaucoma_predictions.txt\n"
     ]
    }
   ],
   "source": [
    "test_dir = os.path.join(path, 'REFUGE', 'train', 'Images')\n",
    "json_path = os.path.join(path, 'REFUGE', 'train', 'index.json')\n",
    "\n",
    "# Threshold for vCDR classification (e.g., images with vCDR > 0.6 are labeled as glaucoma)\n",
    "vCDR_threshold = 0.6\n",
    "test_set = TestDataset(test_dir, json_path)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False)\n",
    "checkpoint = torch.load('unet_model.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "output_file_path = \"glaucoma_predictions.txt\" \n",
    "\n",
    "# Perform predictions and collect data\n",
    "with torch.no_grad():\n",
    "    predictions, ground_truth_labels = [], []\n",
    "    with open(output_file_path, \"w\") as f: \n",
    "        for (img, ground_truth_label), filename in zip(test_loader, test_set.image_filenames):\n",
    "            img = img.to(device)\n",
    "            logits = model(img)\n",
    "\n",
    "            # Get segmentation predictions for OD and OC\n",
    "            pred_od = refine_seg((logits[:, 0, :, :] >= 0.5).type(torch.int8).cpu()).to(device)\n",
    "            pred_oc = refine_seg((logits[:, 1, :, :] >= 0.5).type(torch.int8).cpu()).to(device)\n",
    "            pred_vCDR = vertical_cup_to_disc_ratio(pred_od.cpu().numpy(), pred_oc.cpu().numpy())[0]\n",
    "            predicted_label = int(pred_vCDR > vCDR_threshold)\n",
    "            ground_truth_label = int(ground_truth_label.item())\n",
    "            predictions.append(predicted_label)\n",
    "            ground_truth_labels.append(ground_truth_label)\n",
    "            f.write(\n",
    "                f\"Image: {filename}, vCDR: {pred_vCDR:.2f}, Prediction: {predicted_label}, Ground Truth: {ground_truth_label}\\n\"\n",
    "            )\n",
    "\n",
    "            # Update TP, TN, FP, FN counts\n",
    "            if predicted_label == 1 and ground_truth_label == 1:\n",
    "                tp += 1\n",
    "            elif predicted_label == 0 and ground_truth_label == 0:\n",
    "                tn += 1\n",
    "            elif predicted_label == 1 and ground_truth_label == 0:\n",
    "                fp += 1\n",
    "            elif predicted_label == 0 and ground_truth_label == 1:\n",
    "                fn += 1\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = np.mean([pred == gt for pred, gt in zip(predictions, ground_truth_labels)])\n",
    "\n",
    "with open(output_file_path, \"r+\") as f:\n",
    "    content = f.read()\n",
    "    f.seek(0, 0)\n",
    "    f.write(f\"Overall Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "    f.write(content)\n",
    "\n",
    "print(f\"Test Accuracy for Glaucoma Classification: {accuracy * 100:.2f}%\")\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"Predictions have been saved to {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
